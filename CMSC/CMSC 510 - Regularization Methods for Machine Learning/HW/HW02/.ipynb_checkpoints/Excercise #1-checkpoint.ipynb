{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4932ad1",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64ba4ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd;\n",
    "from scipy.stats import zscore\n",
    "import torch as torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7483ab80",
   "metadata": {},
   "source": [
    "Excercise #1: \n",
    "\n",
    "Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9247a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldataset = pd.read_csv('./iris.csv')\n",
    "\n",
    "np_x=fulldataset[list(fulldataset.columns)[0:-1]].apply(zscore).to_numpy();\n",
    "np_y = pd.get_dummies(fulldataset['variety']).to_numpy();\n",
    "\n",
    "n_classes = 3;\n",
    "n_features = np_x.shape[1];\n",
    "\n",
    "x_train=np_x\n",
    "y_train=np_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ca0e35",
   "metadata": {},
   "source": [
    "Now to create our tensor of input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34bc8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x_train=torch.tensor(x_train,requires_grad=False,dtype=torch.float64,device='cpu');\n",
    "t_y_train=torch.tensor(y_train,requires_grad=False,dtype=torch.float64,device='cpu');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67ce2a",
   "metadata": {},
   "source": [
    "Initialize variables for Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d629f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_std_dev = 0.01;\n",
    "initialW=init_std_dev*np.random.randn(n_features,n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897cab64",
   "metadata": {},
   "source": [
    "Creating variables for weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c96457e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor(initialW,requires_grad=True,device='cpu');\n",
    "b = torch.zeros((1,n_classes),requires_grad=True,device='cpu');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e3106d",
   "metadata": {},
   "source": [
    "I've heard ADAM was the best optimizer so I'm going to stick with that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275aca41",
   "metadata": {},
   "source": [
    "I think the smartest approach to evaluating the different learning rates is to test different learning rates until they reach a specified level of precision in the accuracy of predictions. This way we don't have to modulate the epoch numbers AND the learning rate each trial. We simply have to modulate the learning rate and count the number of iterations to achieve a level of precision.\n",
    "\n",
    "To prevent runaway looping, I've set a limit to the maximum number of iterations before the loop \"gives up\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53c000e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of loop results:\n",
      "Completed in 50 iterations with 95.3333 percent accuracy and an error of 0.0025 on training data.\n",
      "--------------------------------\n",
      "End of loop results:\n",
      "Completed in 143 iterations with 95.3333 percent accuracy and an error of 0.0112 on training data.\n",
      "--------------------------------\n",
      "End of loop results:\n",
      "Completed in 1400 iterations with 95.3333 percent accuracy and an error of 0.0119 on training data.\n",
      "--------------------------------\n",
      "End of loop results:\n",
      "Completed in 8244 iterations with 95.3333 percent accuracy and an error of 0.0123 on training data.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.5, 0.05, 0.005, 0.0005]\n",
    "for rate in learning_rate:\n",
    "    W = torch.tensor(initialW,requires_grad=True,device='cpu');\n",
    "    b = torch.zeros((1,n_classes),requires_grad=True,device='cpu');\n",
    "    optimizer = torch.optim.Adam([W,b],lr=rate)\n",
    "    iteration_limit = 100000; #Desired maximum iterations\n",
    "    tol = 0.95 # Desired Accuracy\n",
    "    i = 0\n",
    "    accuracy = 0\n",
    "    while accuracy < tol and i < iteration_limit:\n",
    "        # clear previous gradient calculations\n",
    "        optimizer.zero_grad();\n",
    "        # calculate model predictions\n",
    "        linear_predictions = torch.matmul(t_x_train,W)+b\n",
    "        activations = 1.0 / (1.0 + torch.exp(-linear_predictions));\n",
    "        #calculate loss\n",
    "        prediction_error = t_y_train-activations\n",
    "        risk = torch.mean(torch.pow(prediction_error,2));\n",
    "        #calculate gradients of risk w.r.t. W,b and propagate them back\n",
    "        risk.backward();\n",
    "        # use the gradient to change W, b\n",
    "        optimizer.step();\n",
    "        # calculate accuracy (on the training set!)\n",
    "        true_class = np.argmax(t_y_train.detach().cpu().numpy(),axis=1)\n",
    "        pred_class = np.argmax(activations.detach().cpu().numpy(),axis=1)\n",
    "        accuracy = np.count_nonzero(true_class == pred_class)/pred_class.shape[0];\n",
    "        prediction_error = np.abs(np.mean(t_y_train.detach().numpy()-activations.detach().numpy()))\n",
    "        i = i+1\n",
    "\n",
    "    print('End of loop results:')\n",
    "    print('Completed in '+str(i)+' iterations with '+str(round(accuracy*100,4))+' percent accuracy and an error of '+str(round(prediction_error,4))+' on training data.')\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764726a7",
   "metadata": {},
   "source": [
    "Aiming for 95% accuracy results in convergence in 50 iterations using the fastest learning rate. Its interesting to note that the fastest learning rate results in the lowest prediction error. \n",
    "\n",
    "This is due to how we've defined the stopping condition of our loop. The slower the learning rate, the more iterations it takes to attain the same level of accuracy. However, the slower the learning rate the more precise the theoretically attainable solution becomes.\n",
    "\n",
    "For example, if the accuracy tolerance is set to 99.9%, accuracy you will see that the fastest learning rate simply cannot attain an accuracy of 99%. However the slower learning rate does posses the granularity attain such performance. As such it is a matter of the programmer's preference to determine what level of precision is required for their problem.\n",
    "\n",
    "Warning: This one will take a bit to go through 100,000 iterations. It will complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "376ab9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of loop results:\n",
      "Completed in 2662 iterations with 98.6667 percent accuracy and an error of 0.009 on training data.\n",
      "--------------------------------\n",
      "End of loop results:\n",
      "Completed in 91323 iterations with 98.6667 percent accuracy and an error of 0.0083 on training data.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.5, 0.0005]\n",
    "for rate in learning_rate:\n",
    "    W = torch.tensor(initialW,requires_grad=True,device='cpu');\n",
    "    b = torch.zeros((1,n_classes),requires_grad=True,device='cpu');\n",
    "    optimizer = torch.optim.Adam([W,b],lr=rate)\n",
    "    iteration_limit = 100000; #Desired maximum iterations\n",
    "    tol = 0.9866 # Desired Accuracy\n",
    "    i = 0\n",
    "    accuracy = 0\n",
    "    while accuracy < tol and i < iteration_limit:\n",
    "        # clear previous gradient calculations\n",
    "        optimizer.zero_grad();\n",
    "        # calculate model predictions\n",
    "        linear_predictions = torch.matmul(t_x_train,W)+b\n",
    "        activations = 1.0 / (1.0 + torch.exp(-linear_predictions));\n",
    "        #calculate loss\n",
    "        prediction_error = t_y_train-activations\n",
    "        risk = torch.mean(torch.pow(prediction_error,2));\n",
    "        #calculate gradients of risk w.r.t. W,b and propagate them back\n",
    "        risk.backward();\n",
    "        # use the gradient to change W, b\n",
    "        optimizer.step();\n",
    "        # calculate accuracy (on the training set!)\n",
    "        true_class = np.argmax(t_y_train.detach().cpu().numpy(),axis=1)\n",
    "        pred_class = np.argmax(activations.detach().cpu().numpy(),axis=1)\n",
    "        accuracy = np.count_nonzero(true_class == pred_class)/pred_class.shape[0];\n",
    "        prediction_error = np.abs(np.mean(t_y_train.detach().numpy()-activations.detach().numpy()))\n",
    "        i = i+1\n",
    "\n",
    "    print('End of loop results:')\n",
    "    print('Completed in '+str(i)+' iterations with '+str(round(accuracy*100,4))+' percent accuracy and an error of '+str(round(prediction_error,4))+' on training data.')\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0324085",
   "metadata": {},
   "source": [
    "As you can see, for this run the algorithm ran for ~91,000 iterations on the slower learning rate. But we have a much lower prediction error. However the computational cost to attain this increased precision does not seem worth it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4932ad1",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ba4ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd;\n",
    "from scipy.stats import zscore\n",
    "import torch as torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7483ab80",
   "metadata": {},
   "source": [
    "Excercise #2: \n",
    "\n",
    "Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9247a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldataset = pd.read_csv('./iris.csv')\n",
    "\n",
    "np_x=fulldataset[list(fulldataset.columns)[0:-1]].apply(zscore).to_numpy();\n",
    "np_y = pd.get_dummies(fulldataset['variety']).to_numpy();\n",
    "\n",
    "n_classes = 3;\n",
    "n_features = np_x.shape[1];\n",
    "\n",
    "x_train=np_x\n",
    "y_train=np_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ca0e35",
   "metadata": {},
   "source": [
    "Now to create our tensor of input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34bc8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x_train=torch.tensor(x_train,requires_grad=False,dtype=torch.float64,device='cpu');\n",
    "t_y_train=torch.tensor(y_train,requires_grad=False,dtype=torch.float64,device='cpu');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67ce2a",
   "metadata": {},
   "source": [
    "Initialize variables for Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d629f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_std_dev = 0.01;\n",
    "initialW=init_std_dev*np.random.randn(n_features,n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897cab64",
   "metadata": {},
   "source": [
    "Creating variables for weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96457e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor(initialW,requires_grad=True,device='cpu');\n",
    "b = torch.zeros((1,n_classes),requires_grad=True,device='cpu');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e3106d",
   "metadata": {},
   "source": [
    "Here lets define our CrossEntropyLoss and Softmax functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "804071f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossModel = nn.CrossEntropyLoss()\n",
    "sm = nn.Softmax(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275aca41",
   "metadata": {},
   "source": [
    "For this experiment, our loop is virtually identical to the one in excercise #1. We are using identical learning rates with an identical accuracy tolerance. The only thing which has changed is our loss function.\n",
    "\n",
    "As such, we can directly compare the speed at which this method converges to the previous. A lower iteration count for the same learning rate means that it converges to the solution faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53c000e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of loop results:\n",
      "Completed in 10 iterations with 95.3333 percent accuracy and an error of 0.275 on training data.\n",
      "--------------------------------\n",
      "End of loop results:\n",
      "Completed in 34 iterations with 95.3333 percent accuracy and an error of 0.1877 on training data.\n",
      "--------------------------------\n",
      "End of loop results:\n",
      "Completed in 328 iterations with 95.3333 percent accuracy and an error of 0.1894 on training data.\n",
      "--------------------------------\n",
      "End of loop results:\n",
      "Completed in 2864 iterations with 95.3333 percent accuracy and an error of 0.2064 on training data.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.5, 0.05, 0.005, 0.0005]\n",
    "for rate in learning_rate:\n",
    "    W = torch.tensor(initialW,requires_grad=True,device='cpu');\n",
    "    b = torch.zeros((1,n_classes),requires_grad=True,device='cpu');\n",
    "    optimizer = torch.optim.Adam([W,b],lr=rate)\n",
    "    iteration_limit = 100000; #Desired maximum iterations\n",
    "    tol = 0.95 # Desired Accuracy\n",
    "    i = 0\n",
    "    accuracy = 0\n",
    "    while accuracy < tol and i < iteration_limit:\n",
    "        # clear previous gradient calculations\n",
    "        optimizer.zero_grad();\n",
    "        # calculate model predictions\n",
    "        linear_predictions = torch.matmul(t_x_train,W)+b\n",
    "        activations = 1.0 / (1.0 + torch.exp(-linear_predictions));\n",
    "        norm_predictions = sm(linear_predictions)\n",
    "        #calculate loss\n",
    "        loss = lossModel(linear_predictions, t_y_train)\n",
    "        risk = torch.mean(loss)\n",
    "        #calculate gradients of risk w.r.t. W,b and propagate them back\n",
    "        loss.backward();\n",
    "        # use the gradient to change W, b\n",
    "        optimizer.step();\n",
    "        # calculate accuracy (on the training set!)\n",
    "        true_class = np.argmax(t_y_train.detach().cpu().numpy(),axis=1)\n",
    "        pred_class = np.argmax(activations.detach().cpu().numpy(),axis=1)\n",
    "        accuracy = np.count_nonzero(true_class == pred_class)/pred_class.shape[0];\n",
    "        prediction_error = np.abs(np.mean(t_y_train.detach().numpy()-activations.detach().numpy()))\n",
    "        i = i+1\n",
    "    print('End of loop results:')\n",
    "    print('Completed in '+str(i)+' iterations with '+str(round(accuracy*100,4))+' percent accuracy and an error of '+str(round(prediction_error,4))+' on training data.')\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c1e0b",
   "metadata": {},
   "source": [
    "Cross entropy loss is lightning fast compared to mean squared error. However its interesting to note that the mean square error is remarkably higher than the previous exercise. This however is an expected result. Optimizing mean squared error directly one would a lower mean squared error.\n",
    "\n",
    "The speed advantage of Cross Entropy Loss makes this version of the model much more viable for high accuracy models. As demonstrated in the final section of Excercise #1, a model which attains 99.9% accuracy was unattainable in less than 100,000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "376ab9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of loop results:\n",
      "Completed in 18007 iterations with 98.6667 percent accuracy and an error of 0.2529 on training data.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "W = torch.tensor(initialW,requires_grad=True,device='cpu');\n",
    "b = torch.zeros((1,n_classes),requires_grad=True,device='cpu');\n",
    "optimizer = torch.optim.Adam([W,b],lr=rate)\n",
    "iteration_limit = 100000; #Desired maximum iterations\n",
    "tol = 0.9866 # Desired Accuracy\n",
    "i = 0\n",
    "accuracy = 0\n",
    "while accuracy < tol and i < iteration_limit:\n",
    "    # clear previous gradient calculations\n",
    "    optimizer.zero_grad();\n",
    "    # calculate model predictions\n",
    "    linear_predictions = torch.matmul(t_x_train,W)+b\n",
    "    activations = 1.0 / (1.0 + torch.exp(-linear_predictions));\n",
    "    norm_predictions = sm(linear_predictions)\n",
    "    #calculate loss\n",
    "    loss = lossModel(linear_predictions, t_y_train)\n",
    "    risk = torch.mean(loss)\n",
    "    #calculate gradients of risk w.r.t. W,b and propagate them back\n",
    "    loss.backward();\n",
    "    # use the gradient to change W, b\n",
    "    optimizer.step();\n",
    "    # calculate accuracy (on the training set!)\n",
    "    true_class = np.argmax(t_y_train.detach().cpu().numpy(),axis=1)\n",
    "    pred_class = np.argmax(activations.detach().cpu().numpy(),axis=1)\n",
    "    accuracy = np.count_nonzero(true_class == pred_class)/pred_class.shape[0];\n",
    "    prediction_error = np.abs(np.mean(t_y_train.detach().numpy()-activations.detach().numpy()))\n",
    "    i = i+1\n",
    "print('End of loop results:')\n",
    "print('Completed in '+str(i)+' iterations with '+str(round(accuracy*100,4))+' percent accuracy and an error of '+str(round(prediction_error,4))+' on training data.')\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0fb62",
   "metadata": {},
   "source": [
    "As you can see above, we are able to attain 98.6% accuracy in only ~18,000 iterations. Less than half the iterations to achieve the same accuracy as mean square error. Despite the lower prediciction accuracy this speed advantage is probably more than worth the loss in prediction error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4932ad1",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ba4ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd;\n",
    "from scipy.stats import zscore\n",
    "import torch as torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7483ab80",
   "metadata": {},
   "source": [
    "Excercise #3: \n",
    "\n",
    "Batching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9247a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldataset = pd.read_csv('./iris.csv')\n",
    "\n",
    "np_full_input=fulldataset[list(fulldataset.columns)[0:-1]].apply(zscore).to_numpy();\n",
    "np_full_output = pd.get_dummies(fulldataset['variety']).to_numpy();\n",
    "\n",
    "\n",
    "n_classes = 3;\n",
    "n_features = np_full_input.shape[1];\n",
    "\n",
    "x_train=np_full_input[0:99,:]\n",
    "y_train=np_full_output[0:99,:]\n",
    "x_train.shape\n",
    "\n",
    "#Partition off 1/3rd of the dataset for testing\n",
    "x_test = np_full_input[100:149,:]\n",
    "y_test = np_full_input[100:149,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ca0e35",
   "metadata": {},
   "source": [
    "Now to create our tensor of training and testing variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34bc8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x_train=torch.tensor(x_train,requires_grad=False,dtype=torch.float64,device='cpu');\n",
    "t_y_train=torch.tensor(y_train,requires_grad=False,dtype=torch.float64,device='cpu');\n",
    "t_x_test=torch.tensor(x_train,requires_grad=False,dtype=torch.float64,device='cpu');\n",
    "t_y_test=torch.tensor(y_train,requires_grad=False,dtype=torch.float64,device='cpu');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baf329e",
   "metadata": {},
   "source": [
    "Lets divide this training data into 5 batches of size 20 each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4d4c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_batches = math.ceil(t_x_test.size()[0]/batch_size)\n",
    "x_test_batches = [t_x_test[batch_size*j:batch_size*(j+1),:] for j in range(num_batches)]\n",
    "y_test_batches = [t_y_test[batch_size*j:batch_size*(j+1),:] for j in range(num_batches)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67ce2a",
   "metadata": {},
   "source": [
    "Initialize variables for Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d629f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_std_dev = 0.01;\n",
    "initialW=init_std_dev*np.random.randn(n_features,n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897cab64",
   "metadata": {},
   "source": [
    "Creating variables for weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c96457e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor(initialW,requires_grad=True,device='cpu');\n",
    "b = torch.zeros((1,n_classes),requires_grad=True,device='cpu');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e3106d",
   "metadata": {},
   "source": [
    "Here lets define our CrossEntropyLoss and Softmax functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "804071f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossModel = nn.CrossEntropyLoss()\n",
    "sm = nn.Softmax(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275aca41",
   "metadata": {},
   "source": [
    "Lets run a loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53c000e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of loop results:\n",
      "Completed in 86 iterations with 100.0 percent accuracy on testing data and an error of 0.1633 on training data.\n",
      "--------------------------------\n",
      "End of loop results:\n",
      "Completed in 127 iterations with 100.0 percent accuracy on testing data and an error of 0.1632 on training data.\n",
      "--------------------------------\n",
      "End of loop results:\n",
      "Completed in 125 iterations with 96.0 percent accuracy on testing data and an error of 0.1631 on training data.\n",
      "--------------------------------\n",
      "End of loop results:\n",
      "Completed in 137 iterations with 96.0 percent accuracy on testing data and an error of 0.1631 on training data.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "rate = 0.00005\n",
    "batch_sizes = [5,10,25,50]\n",
    "for batch_size in batch_sizes:\n",
    "    W = torch.tensor(initialW,requires_grad=True,device='cpu');\n",
    "    b = torch.zeros((1,n_classes),requires_grad=True,device='cpu');\n",
    "    num_batches = math.ceil(t_x_test.size()[0]/batch_size)\n",
    "    x_test_batches = [t_x_test[batch_size*j:batch_size*(j+1),:] for j in range(num_batches)]\n",
    "    y_test_batches = [t_y_test[batch_size*j:batch_size*(j+1),:] for j in range(num_batches)]\n",
    "    batch_x = x_test_batches[0]\n",
    "    batch_y = y_test_batches[0]\n",
    "    optimizer = torch.optim.Adam([W,b],lr=rate)\n",
    "    iteration_limit = 100000; #Desired maximum iterations\n",
    "    tol = 0.95 # Desired Error\n",
    "    i = 0\n",
    "    accuracy = 0\n",
    "    while accuracy < tol and i < iteration_limit:\n",
    "        # clear previous gradient calculations\n",
    "        optimizer.zero_grad();\n",
    "        # Load one of n batches\n",
    "        batch_number = random.randint(0,num_batches-1)\n",
    "        batch_x = x_test_batches[batch_number]\n",
    "        batch_y = y_test_batches[batch_number]\n",
    "        # calculate model predictions\n",
    "        linear_predictions = torch.matmul(batch_x,W)+b\n",
    "        activations = 1.0 / (1.0 + torch.exp(-linear_predictions));\n",
    "        norm_predictions = sm(linear_predictions)\n",
    "        #calculate loss\n",
    "        loss = lossModel(linear_predictions, batch_y)\n",
    "        risk = torch.mean(loss)\n",
    "        #calculate gradients of risk w.r.t. W,b and propagate them back\n",
    "        loss.backward();\n",
    "        # use the gradient to change W, b\n",
    "        optimizer.step();\n",
    "        #calculate accuracy (on the training set!)\n",
    "        true_class = np.argmax(batch_y.detach().cpu().numpy(),axis=1)\n",
    "        pred_class = np.argmax(activations.detach().cpu().numpy(),axis=1)\n",
    "        accuracy = np.count_nonzero(true_class == pred_class)/pred_class.shape[0];\n",
    "        i = i+1\n",
    "    # calculate error (on the testing set!)\n",
    "    linear_predictions = torch.matmul(t_x_test,W)+b\n",
    "    activations = 1.0 / (1.0 + torch.exp(-linear_predictions));\n",
    "    prediction_error = np.abs(np.mean(t_y_test.detach().numpy()-activations.detach().numpy()))\n",
    "    print('End of loop results:')\n",
    "    print('Completed in '+str(i)+' iterations with '+str(round(accuracy*100,4))+' percent accuracy on training data and an error of '+str(round(prediction_error,4))+' on testing data.')\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa29977",
   "metadata": {},
   "source": [
    "I found while running the model through the testing data. None of the models gave particularly good accuracy. I believe this is because in a real world application, the likleyhood of a linear model matching the input and output of a dataset exactly are incredibly low. Before partitioning our dataset, we could use accuracy as a metric since we are directly fitting our model to the dataset, but after dividing it we can't expect perfect accuracy on the testing data.\n",
    "\n",
    "As a result, the prediction error must now be used as our metric to evaluate the \"goodness\" of our linear models.\n",
    "\n",
    "It seems that Batching slows down convergence to the actual value somewhat. However oddly enough, it enhances the precision of the model compared to using one large batch as the dataset. As you can see we have attained 100% accuracy and much lower prediction error compared to training the model on one large batch.\n",
    "\n",
    "I suspect this precision increase has more to do with partitioning the dataset between testing and trading data than batching. By setting aside some data for testing, we help avoid overfitting of our approximation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

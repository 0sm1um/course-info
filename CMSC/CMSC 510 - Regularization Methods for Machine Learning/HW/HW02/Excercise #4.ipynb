{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4932ad1",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ba4ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd;\n",
    "from scipy.stats import zscore\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7483ab80",
   "metadata": {},
   "source": [
    "Excercise #4: \n",
    "\n",
    "Larger Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9247a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 784 # number of pixels\n",
    "n_classes = 10\n",
    "full_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "full_test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "x_train = full_train_dataset.data.numpy().reshape(-1,n_features).astype(dtype=float)/255.0;\n",
    "x_test = full_test_dataset.data.numpy().reshape(-1,n_features).astype(dtype=float)/255.0;\n",
    "y_train = full_train_dataset.targets.numpy()\n",
    "y_test = full_test_dataset.targets.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ca0e35",
   "metadata": {},
   "source": [
    "Now to create our tensor of training and testing variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34bc8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x_train=torch.tensor(x_train,requires_grad=False,dtype=torch.float64,device='cpu');\n",
    "t_y_train=torch.tensor(y_train,requires_grad=False,dtype=torch.long,device='cpu');\n",
    "t_x_test=torch.tensor(x_train,requires_grad=False,dtype=torch.float64,device='cpu');\n",
    "t_y_test=torch.tensor(y_train,requires_grad=False,dtype=torch.long,device='cpu');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67ce2a",
   "metadata": {},
   "source": [
    "Initialize variables for Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d629f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_std_dev = 0.01;\n",
    "initialW=init_std_dev*np.random.randn(n_features,n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897cab64",
   "metadata": {},
   "source": [
    "Creating variables for weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96457e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor(initialW,requires_grad=True,dtype=torch.float64,device='cpu');\n",
    "b = torch.zeros((1,n_classes),requires_grad=True,dtype=torch.float64,device='cpu');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e3106d",
   "metadata": {},
   "source": [
    "Here lets define our CrossEntropyLoss and Softmax functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "804071f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossModel = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275aca41",
   "metadata": {},
   "source": [
    "For this experiment, lets use a fast learning rate and a low accuracy tolerance. This dataset is quite large so I want to ensure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c000e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of loop results:\n",
      "Completed in 10000 iterations with an error of 4.1076 on training data.\n",
      "--------------------------------\n",
      "End of loop results:\n",
      "Completed in 10000 iterations with an error of 4.0674 on training data.\n",
      "--------------------------------\n",
      "End of loop results:\n",
      "Completed in 10000 iterations with an error of 3.9395 on training data.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "rate = 0.5\n",
    "batch_sizes = [100,500,1000,10000, 60000]\n",
    "for batch_size in batch_sizes:\n",
    "    W = torch.tensor(initialW,requires_grad=True,device='cpu');\n",
    "    b = torch.zeros((1,n_classes),requires_grad=True,device='cpu');\n",
    "    num_batches = math.ceil(t_x_test.size()[0]/batch_size)\n",
    "    x_train_batches = [t_x_train[batch_size*j:batch_size*(j+1),:] for j in range(num_batches)]\n",
    "    y_train_batches = [t_y_train[batch_size*j:batch_size*(j+1)] for j in range(num_batches)]\n",
    "    x_test_batches = [t_x_test[batch_size*j:batch_size*(j+1),:] for j in range(num_batches)]\n",
    "    y_test_batches = [t_y_test[batch_size*j:batch_size*(j+1)] for j in range(num_batches)]\n",
    "    batch_x = x_train_batches[0]\n",
    "    batch_y = y_train_batches[0]\n",
    "    optimizer = torch.optim.Adam([W,b],lr=rate)\n",
    "    iteration_limit = 10000; #Desired maximum iterations\n",
    "    tol = 0.85 # Desired Error\n",
    "    i = 0\n",
    "    accuracy = 0\n",
    "    while accuracy < tol and i < iteration_limit:\n",
    "        # clear previous gradient calculations\n",
    "        optimizer.zero_grad();\n",
    "        # Load one of n batches\n",
    "        batch_number = random.randint(0,num_batches-1)\n",
    "        batch_x = x_test_batches[batch_number]\n",
    "        batch_y = y_test_batches[batch_number]\n",
    "        # calculate model predictions\n",
    "        linear_predictions = torch.matmul(batch_x,W)+b\n",
    "        activations = 1.0 / (1.0 + torch.exp(-linear_predictions));\n",
    "        #calculate loss\n",
    "        loss = lossModel(linear_predictions, batch_y)\n",
    "        risk = torch.mean(loss)\n",
    "        #calculate gradients of risk w.r.t. W,b and propagate them back\n",
    "        loss.backward();\n",
    "        # use the gradient to change W, b\n",
    "        optimizer.step();\n",
    "        #calculate accuracy (on the training set!)\n",
    "        #true_class = np.argmax(batch_y.detach().cpu().numpy())\n",
    "        #pred_class = np.argmax(activations.detach().cpu().numpy(),axis=1)\n",
    "        #accuracy = np.count_nonzero(true_class == pred_class)/pred_class.shape[0];\n",
    "        i = i+1\n",
    "    # calculate error (on the testing set!)\n",
    "    linear_predictions = torch.matmul(t_x_test,W)+b\n",
    "    activations = 1.0 / (1.0 + torch.exp(-linear_predictions));\n",
    "    prediction_error = np.abs(np.mean(t_y_test.detach().numpy().reshape([len(t_y_test),1])-activations.detach().numpy()))\n",
    "    print('End of loop results:')\n",
    "    print('Completed in '+str(i)+' iterations with an error of '+str(round(prediction_error,4))+' on training data.')\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa29977",
   "metadata": {},
   "source": [
    "The batch size doesn't seem extremley important in terms of the accuracy. Increasing it doesn't result in significantly better or worse prediction error. I suspect that this dataset might be large enough to the point where even with a fast learning rate it might take more iterations than I have allotted to get a precise or accurate linear classifier.\n",
    "\n",
    "As it stands 10,000 iterations with this new dataset is quite long.\n",
    "\n",
    "I have however noticed that the larger the batch size, the faster the loop goes(the less time it takes to complete each iteration). This makes sense considering the memory to speed tradeoff. This difference in computational time was not noticable on the IRIS dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

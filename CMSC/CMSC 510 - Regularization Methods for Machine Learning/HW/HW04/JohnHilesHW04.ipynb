{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4932ad1",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ba4ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "from scipy.stats import zscore\n",
    "import torch as torch;\n",
    "import numpy as np\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn;\n",
    "import torch.nn.functional as F;\n",
    "np.random.seed(42)\n",
    "# Use GPU if available, else use CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7483ab80",
   "metadata": {},
   "source": [
    "# HW04: Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ca0e35",
   "metadata": {},
   "source": [
    "I was unable to sucsessfully implement a working accuracy metric. As a result, to avoid being totally unable to complete the project I rewrote my project using the keras interface and built in functions despite the fact this was not the point of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658aed68",
   "metadata": {},
   "source": [
    "Data Pre Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9247a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "num_classes=10;\n",
    "transform = transforms.Compose(\n",
    "        [transforms.Resize((224, 224)),\n",
    "         transforms.RandomResizedCrop(224),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])])\n",
    "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, \n",
    "                             transform=transform)\n",
    "full_test_dataset = datasets.CIFAR10(root='./data', train=False, download=True,\n",
    "                             transform=transform)\n",
    "batch_size=64;\n",
    "trainloader = torch.utils.data.DataLoader(full_train_dataset,batch_size=batch_size,shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(full_test_dataset,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909cefda",
   "metadata": {},
   "source": [
    "Here we define the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84ea2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a neural network (inherit from nn.Module)\n",
    "class ConvNetWithBatchNorm(nn.Module):\n",
    "    # architecture of the network is specified in the constructor\n",
    "    def __init__(self): \n",
    "        super(ConvNetWithBatchNorm, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),         \n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),  \n",
    "            nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3),\n",
    "            nn.BatchNorm2d(num_features=12)           \n",
    "        )\n",
    "        self.features1 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)   \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(12*6*6, 50),         \n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,num_classes)            \n",
    "        )\n",
    "        \n",
    "    # here we specify the computation (forward phase of training) how \"x\" is transfered into output \"y\"\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.features1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337c8bd3",
   "metadata": {},
   "source": [
    "Here we instantiate the neural network, set our learning rate, and instantiate our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89664df5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3910\n"
     ]
    }
   ],
   "source": [
    "model=ConvNetWithBatchNorm().to(device);\n",
    "criterion = F.nll_loss;\n",
    "# this optimizer will do gradient descent for us\n",
    "# experiment with learning rate and optimizer type\n",
    "learning_rate = 0.001;\n",
    "# note that we have to add all weights&biases, for both layers, to the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "# we add a learning rate scheduler, which will modify the learning rate during training\n",
    "# will initially start low, then increase it (\"warm up\"), and then gradually decrease it\n",
    "n_epochs = 5;\n",
    "num_updates = n_epochs*int(np.ceil(len(trainloader.dataset)/batch_size))\n",
    "print(num_updates)\n",
    "warmup_steps=1000;\n",
    "def warmup_linear(x):\n",
    "    if x < warmup_steps:\n",
    "        lr=x/warmup_steps\n",
    "    else:\n",
    "        lr=max( (num_updates - x ) / (num_updates - warmup_steps), 0.)\n",
    "    return lr;\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, warmup_linear);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89e53e1",
   "metadata": {},
   "source": [
    "Next we have our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a61c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_epochs):\n",
    "    for j, data in enumerate(trainloader):\n",
    "\n",
    "        inputs, labels = data        \n",
    "        inputs=inputs.to(device);\n",
    "        labels=labels.to(device);\n",
    "        \n",
    "        optimizer.zero_grad();\n",
    "        #forward phase - predictions by the model\n",
    "        outputs = model(inputs);\n",
    "        #forward phase - risk/loss for the predictions\n",
    "        risk = criterion(outputs, labels);\n",
    "\n",
    "        # calculate gradients\n",
    "        risk.backward();\n",
    "\n",
    "        # take the gradient step\n",
    "        optimizer.step();\n",
    "        scheduler.step();\n",
    "\n",
    "        batch_risk=risk.item();\n",
    "    with (torch.no_grad()):\n",
    "      correct = 0;\n",
    "      for j, data in enumerate(testloader):\n",
    "\n",
    "          inputs, labels = data\n",
    "          inputs=inputs.to(device);\n",
    "          labels=labels.to(device);\n",
    "          outputs = model(inputs);\n",
    "          pred = outputs.data.max(dim=1, keepdim=True)[1]\n",
    "          correct += pred.eq(labels.data.view_as(pred)).sum().item();\n",
    "    print(i, batch_risk, correct / len(testloader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88f32ea",
   "metadata": {},
   "source": [
    "Next we need to instantiate our ResNet18 network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34bc8e93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\echo4\\.conda\\envs\\pt\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\echo4\\.conda\\envs\\pt\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "ResNet18model = models.resnet18(pretrained=False)   #load resnet18 model\n",
    "num_features = ResNet18model.fc.in_features     #extract fc layers features\n",
    "ResNet18model.fc = nn.Linear(num_features, num_classes) \n",
    "ResNet18model = ResNet18model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_epochs):\n",
    "    for j, data in enumerate(trainloader):\n",
    "\n",
    "        inputs, labels = data        \n",
    "        inputs=inputs.to(device);\n",
    "        labels=labels.to(device);\n",
    "        \n",
    "        optimizer.zero_grad();\n",
    "        #forward phase - predictions by the model\n",
    "        outputs = ResNet18model(inputs);\n",
    "        #forward phase - risk/loss for the predictions\n",
    "        risk = criterion(outputs, labels);\n",
    "\n",
    "        # calculate gradients\n",
    "        risk.backward();\n",
    "\n",
    "        # take the gradient step\n",
    "        optimizer.step();\n",
    "        scheduler.step();\n",
    "\n",
    "        batch_risk=risk.item();\n",
    "    with (torch.no_grad()):\n",
    "      correct = 0;\n",
    "      for j, data in enumerate(testloader):\n",
    "\n",
    "          inputs, labels = data\n",
    "          inputs=inputs.to(device);\n",
    "          labels=labels.to(device);\n",
    "          outputs = ResNet18model(inputs);\n",
    "          pred = outputs.data.max(dim=1, keepdim=True)[1]\n",
    "          correct += pred.eq(labels.data.view_as(pred)).sum().item();\n",
    "    print(i, batch_risk, correct / len(testloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed0c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

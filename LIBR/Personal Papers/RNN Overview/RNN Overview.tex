%This is my super simple Real Analysis Homework template

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{geometry}
\geometry{legalpaper, margin=1.5in}

\title{RNN Example}
\author{John Hiles}
\date\today
%This information doesn't actually show up on your document unless you use the maketitle command below

\begin{document}
\maketitle %This command prints the title based on information entered above

%Section and subsection automatically number unless you put the asterisk next to them.


\section{Introduction}

Here I want to introduce the mathematical intuition behind RNNs. But first I must introduce the reader to other network structures.

Most supervised learning problems are derived from linear classifiers.

\subsection*{Single Layer Perception}

The objective of a linear classifier is to input a feature vector and decide if it belongs to one class or another.

For this example we will refer to binary classification.

Fundamentally speaking, how a linear classifier operates is a very simple decision rule. Does the input lie above or below the best fit line?

In two dimensions the linear classifier can be easily understood as whether or not an input $x$ lies above or below the best fit line.

\begin{align*}
f(x) = 
 \begin{cases} 
      1 & x > mx \\
      0 & x \leq mx \\
   \end{cases}
\end{align*}

However a more rigorous definition of linear classifiers is necessary for our purposes. The broadest definition would be:

\begin{align*}
y = f(\sum_{i=1}^{n} w_i x_i)
\end{align*}

In this formalism, y is the output of the classifier, and this can be either a continuous real output, or restricted according to the function $f$. $f$ is called the activation function, and it is meant to encode the decision making process of the classifier. The activation function operates on a linear combination of variables. For the rest of this, we will discuss the case where
\begin{align*}
f(x) = sign(x) =
 \begin{cases} 
      1 & x > 0 \\
      0 & x \leq 0 \\
   \end{cases}
\end{align*}

Another more compact way of writing this using the dot product would be:
\begin{align*}
\hat{x}_i = sign(\mathbf{w} \cdot \mathbf{x}_i)
\end{align*}
This particular type of linear classifier is known as a "Linear Perceptron".

Also note that the key ingredient which makes this a classifier is the activation function $f(x)=sign(x)$. If instead we wish to set $f(x)=1$, then we have an ordinary least squares estimator.


Now this only tells us about the desired format for the classifier itself, but it doesn't tell us how to arrive at the optimal $\mathbf{w}$ vector suitable for our problem.

Arriving at this $\mathbf{w}$ vector is the core pursuit of machine learning. For the purposes of this tutorial we will exclusively deal with optimization with respect to the Mean Square Error objective function, but other loss functions exist with other quirks and advantages. Mean Square Error is given by

\begin{align*}
f_{MSE}(x_i) = \tfrac{1}{n} \sum_{i=1}^{n} (\hat{x}_i - y_i)^2
\end{align*}
Where $y_i$ represents the "correct" label for a given data point.

Substituting in our predictor directly:

Note here that $\hat{x}$ refers to predictions made by the predictor, and the subscript $i$ refers to individual points on the dataset which is being considered. This error function essentially operates on the dataset being considered. Most machine learning guides gloss over what is going on with the optimization at this step, but for this I won't. Lets look at our A matrix, in the case of a  dimensional input $\mathbf{z}$.
\begin{align*}
\mathbf{w} = <w_1 & w_2> \\ 	\mathbf{y} = <z_1 & z_2>
\end{align*}

Our classifier output becomes:
\begin{align*}
\hat{x} = sign( <w_1 & w_2> \cdot \begin{bmatrix} z_1 \\ z_2 \end{bmatrix})
\end{align*}
\begin{align*}
\hat{x} = sign( w_1z_1 + w_2 z_2 )
\end{align*}

And then our MSE criterion becomes:

\begin{align*}
(sign(w_1z_1 + w_2 z_2) - y_i)^2
\end{align*}

To optimize this with respect to mean square error, we would perform gradient descent on this matrix and we would need to know the partial derivatives of each of these elements with respect to the error function.

Note that $y_i$ represents the "correct" label, which in this case is either $1$ or $0$.

To optimize this error function, we must figure out the partial derivatives of $w_1$ and $w_2$ with respect to this error function.

\begin{align*}
\nabla f_{MSE}(x_i) = \frac{\partial f_{MSE}(x_{i(1)})}{\partial w_1} + \frac{\partial f_{MSE}(x_{i(2)})}{\partial w_2}
\end{align*}

These partial derivatives are analytically solvable for for a single layer perception.
\begin{align*}
\nabla f_{MSE}(x_i) = -2\mathbf{x}_i^T \mathbf{y}_i + 2\mathbf{x}_i^T \mathbf{x} \mathbf{w}
\end{align*}

However in more advanced networks, they are not. However this kind of classifier is more or less the basis of modern neural networks and it is important to understand the through line between it and more advanced structures.

The Linear Perceptrons break down under certain problems, such as the XOR classification problem and they cannot predict or classify on systems which are separated nonlinearly.



\section{Multi Layer Perceptron}

The generalized equation for 2 layer MLP. Note here that H denotes the output of a "hidden layer". And O represents the output of the final layer.

\begin{align*}
H = \sigma(XW^{1}) 
\end{align*}

\begin{align*}
O = \sigma(HW^{2}) 
\end{align*}

To increase the number of layers, all we need to do is create more weight vectors and do another set of inner products.

\begin{align*}
H^{(1)} = \sigma(XW^{(1)}) 
\end{align*}

\begin{align*}
H^{(2)} = \sigma(H^{(1)}W^{(2)})
\end{align*}

\begin{align*}
O = \sigma(H^{(2)}W^{2}) 
\end{align*}


\section{Convolutional Neural Network}

\subsection{Residual Network (ResNet)}

\section{Recurrent Neural Network}
\subsection{Natural Language Processing}
A sequence can be defined as a series of input vectors.

\begin{align*}
\mathbf{x_1}, \mathbf{x_2}, ... , \mathbf{x_t}
\end{align*}

The motivation for this was from natural language processing. Language can be modeled as a semi nth order markov process. Words are encoded into these vectors via a process called tokenization. For the purposes of this discussion, it is only necessary to understand that tokenization involves turning text into sequences of numbers.

But by modeling text in this manner, it allows us to conceptualize the process of translation or predicting the next word in a sentence as a prediction problem.

Given some text, what is the most likley next word in the sentence?

A simple example would be the simple sentence "Do you ever wonder why we are here?"

This sentence would be broken up as a sequence of words, and those words would be tokenized according to some scheme.

\begin{enumerate}
\item[$x_1$] = Do
\item[$x_2$] = you
\item[$x_3$] = ever
\item[$x_4$] = wonder
\item[$x_5$] = why
\item[$x_6$] = we
\item[$x_7$] = are
\item[$x_8$] = here?
\end{enumerate}

Given our own intuition about language, we know that depending on where they are in a sentence, some words are more likely to appear than others. For example, the word "wonder" would likley be a lot less likley to be the start of the sentance. And since "Do" is capitalized, it is a near certainty to be at the start of a sentence, and "here?" has a question mark, meaning its likely the end.

Modeling language as a sequence turns this into a prediction problem. This here is the first order factorization.

\begin{align*}
P(\mathbf{x_1},...,\mathbf{x_t}) = P(\mathbf{x_1}) \prod_{i=1}^{T} P(\mathbf{x_T}|x_{t-1})
\end{align*}

The likleyhood of our previoous sentence would then be represented:
\begin{align*}
P(\text{Do you ever wonder why we are here?}) = \\ P(Do) P(you|Do) P(ever|Do,you) P(wonder|Do,you,ever) \\ P(why|Do,you,ever,wonder) P(we|Do,you,ever,wonder,why) \\ P(we|are,you,ever,wonder,why,we) \\ P(here?|are,you,ever,wonder,why,we,are)
\end{align*}

Note that modeling text in this manner relies on many many assumptions. This is a first order markov model which may or may not be desirable. However raising the order comes with significant increases in computational complexity, and diminishing returns. The other main assumption is modelling it in this manner relies primarily on the frequency of words to calculate these relative likelihoods. However introducing natural language processing I feel is important to demonstrate why time series/temporal correlation is so important and why RNNs were developed to deal with this particular problem.

The intuition behind RNNs is that a network structure was desired which could accurately model systems in which the next step or steps in the sequence were highly correlated with what comes before.

Note here that the input and output of an RNN is somewhat different from the CNN.

The CNN accepts one feature vector $\mathbf{x}$, RNNs accept sequences of vectors $\mathbf{x_1},...,\mathbf{x_t}$ and output sequences of vectors $\mathbf{\hat{x}_1},...,\mathbf{\hat{x}_t}$. On one hand, you might think this distinction is irrelevant, as its simply a matter of interpreting or transforming data. However this is not necessarily the case. The model itself is autoregressive and depends on itself.

So what makes a sequence model a sequence model is that its output sequence depends on itself while in a CNN or MLP, it does not.

\subsection{RNN}

At this point it is important to make a distinction between the MLP and the RNN. In the MLP, information only flows forward which makes it not useful for autoregressive type modeling. The special sauce for an RNN is its ability to transmit information backwards in the neural network.

Recall the MLP is given by:
\begin{align*}
H^{(1)} = \sigma(XW^{(1)}) 
\end{align*}

\begin{align*}
H^{(2)} = \sigma(H^{(1)}W^{(2)})
\end{align*}

\begin{align*}
O = \sigma(H^{(2)}W^{2}) 
\end{align*}

At each layer of the RNN, information is fed forward, but its also sent to a "Hidden State". The hidden state is somewhat nebulously defined as some function of the output of a hidden layer and the current time step.

\begin{align*}
h_{t} = f(x_t, h_{t-1})
\end{align*}

\begin{align*}
H_{t} = \sigma( X_t W_{xh} + H_{t-1} W_{hh} )
\end{align*}

Note this $W_hh$ is the new thing. This didn't exist before and it had no analogue. But it is a new weight to optimize with respect to.


\subsection*{Long Short Term Memory}

\section{Multi Head Attention}

\subsection{Transformer}



\end{document}
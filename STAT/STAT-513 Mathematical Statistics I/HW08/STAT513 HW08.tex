\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb,amsmath,amsthm}

\title{STAT-513 Homework 8}
\author{John Hiles}
\date\today

\begin{document}
\maketitle %This command prints the title based on information entered above


\section*{Problem 1}
Let X be a random variable for which $E(X^2)$ and $E(|X|)$ are defined, and let b>0 be some parameter. We then have the following two inequalities:
\begin{align*}
P(|X|>b) \leq \frac{E(X^2)}{b^2} && P(|X|>b) \leq \frac{E(|X|)}{b}
\end{align*}

\begin{enumerate}
\item[a.] Calculate $E(X^2)$ and $E(|X|)$.
\item[b.] Calculate Which two inequalities give better bound when $b=3$? How about when $b=\sqrt{2}$. Note: the better bound is the one that is smaller.
\item[c.] Calculate $P(|X| > b)$ exactly when $X ~ Exponential(\beta =1)$. Note: in most cases you will never know
this exactly; the point here is that having two inequalities is better than one.
\end{enumerate}
\subsection*{Part A}
\begin{align*}
\mathbb{E}(X^2) = \int_{0}^{\infty} x^2 \frac{1}{\beta} e^{-\frac{x}{\beta}} dx
\end{align*}
Substituting in $\beta=1$
\begin{align*}
\mathbb{E}(X^2) = \int_{0}^{\infty} x^2 e^{-x} dx = -(x^2+2x+2) e^{-x}|_{0}^{\infty} = 2
\end{align*}
Now lets find $\mathbb{E}(|X|)$
\begin{align*}
\mathbb{E}(|X|) = \int_{0}^{\infty} |x| e^{-x} dx
\end{align*}
Via integration by parts:
\begin{align*}
\mathbb{E}(|X|) = -e^{-x} - \int - |x| e^{-x} dx = -e^{-x} - \frac{xe^{-x}}{|x|} |_{0}^{\infty}
\end{align*}
\begin{align*}
\mathbb{E}(|X|) = 1
\end{align*}
\subsection*{Part B}
Let $b=3$. Our inequalities become:
\begin{align*}
P(|X|>3) \leq \frac{\mathbb{E}(X^2)}{3^2} && P(|X|>3) \leq \frac{\mathbb{E}(|X|)}{3}
\end{align*}
Substituting in our expectation values calculated in the first part:
\begin{align*}
P(|X|>3) \leq \frac{2}{9} && P(|X|>3) \leq \frac{1}{3}
\end{align*}
As such, $\boxed{ P(|X|>3) \leq \frac{2}{9} }$ is the better bound for $b=3$.

Let $b=\sqrt{2}$. Our inequalities become:
\begin{align*}
P(|X|>\sqrt{2}) \leq \frac{2}{2} && P(|X|>\sqrt{2}) \leq \frac{1}{\sqrt{2}}
\end{align*}
The second inequality is the better bound for $b=\sqrt{2}$. The first inequality gives a bound of $P(|X|>\sqrt{2}) \leq 1$ basically gives us no information.
\subsection*{Part C}
\begin{align*}
P(|X|>b) = \int_{b}^{\infty} f(x) dx = \int_{b}^{\infty} e^{-x} dx
\end{align*}

\begin{align*}
P(|X|>b) = -e^{-x}|_{b}^{\infty} = -e^{-\infty} - (-e^{-b}) = 0 + e^{-b}
\end{align*}

\begin{align*}
\boxed{P(|X|>b) = e^{-b}}
\end{align*}


\pagebreak
\section*{Problem 2}
Show that the beta distribution with pdf
\begin{align*}
f(x|\alpha,\beta) = \frac{1}{Beta(\alpha,\beta)} x^{\alpha-1} (1-x)^{\beta-1} \text{, } 0<x<1
\end{align*}
is a member of the exponential family.

\subsection*{Part A}
A pdf can be said to be part of the exponential family if it can be represented in the following format:
\begin{align*}
f(x|\theta) = h(x) c(\theta) e^{\sum_{1}^{k} w_i(\theta) t_i(x)}
\end{align*}
Note that $\theta$ can be a vector valued quantity, it just can't depend on $x$ itself. As such, we can define a vector $\theta = (\alpha, \beta)$
We can now rewrite 
\begin{align*}
f(x|\alpha,\beta) = \frac{1}{Beta(\alpha,\beta)} x^{\alpha-1} (1-x)^{\beta-1} \text{, } 0<x<1
\end{align*}
Let $c(\theta) = \frac{1}{Beta(\alpha,\beta)}$
\begin{align*}
f(x|\alpha,\beta) = c(\theta) x^{\alpha-1} (1-x)^{\beta-1} \text{, } 0<x<1
\end{align*}

\begin{align*}
f(x|\alpha,\beta) = c(\theta) e^{(\alpha-1)ln(x)(\beta-1)ln(x)} \text{, } 0<x<1
\end{align*}

\section*{Problem 3}
Suppose X is a random variable with pdf
\begin{align*}
f(x) = \frac{1}{\gamma(\theta+1)} e^{-x} x^\theta \text{, } x>0 \text{, } \theta>0
\end{align*}

Show that

\begin{align*}
P [0 < X < 2(\theta + 1)] > \frac{\theta}{\theta+1}
\end{align*}
\subsection*{Part A}

\section*{Problem 4}
Verify that the N($\theta$,$\theta$) family is an exponential family. Is it a full exponential or a
curved exponential family? Explain your answer.
\subsection*{Part A}

\section*{Problem 5}
The joint pdf of $(X,Y)$ is defined by
\begin{align*}
f_{X,Y}(x,y) = c(2x+y) \text{ for 0<x<2 & 0<y<1}
\end{align*}
\subsection*{Part A}

\section*{Problem 6}
Let $X_1, X_2, X_3$ be uncorrelated random variables, each with mean $\mu$ and variance
$\sigma^2$

Find in terms of $\mu$ and $\sigma^2$, Cov($X_1+X_2$,$X_1+X_3$) and Cov($X_1+X_2$,$X_1-X_2$)


\subsection*{Part A}

\section*{Problem 7}
\begin{align*}
f(y_1,y_2) = [1-\alpha{}(1-2e^{-y_1})(1-2e^{y_2})]e^{-y_1-y_2} && y_1\geq 0 Y_2\geq 0
\end{align*}


for $-1 \leq \alpha \leq 1$.
\begin{enumerate}

\item[a.] Show that the marginal distributions of $Y_1$ and $Y_2$ are both exponential with mean 1.
\item[b.] Show that $Y_1$ and $Y_2$ are independent if and only if $\alpha$ = 0.
\item[c.] Derive Cov($Y_1$, $Y_2$).
\item[d.] Show that Cov($Y_1$, $Y_2$) = 0 if and only if $\alpha$ = 0.
\item[e.] Argue that $Y_1$ and $Y_2$ are independent if and only if $\rho$ = 0.
\end{enumerate}
\subsection*{Part A}

\end{document} 

%This is my super simple Real Analysis Homework template

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{geometry}
\geometry{legalpaper, margin=1.5in}

\title{STAT-514 Homework 2}
\author{John Hiles}
\date\today
%This information doesn't actually show up on your document unless you use the maketitle command below

\begin{document}
\maketitle %This command prints the title based on information entered above

%Section and subsection automatically number unless you put the asterisk next to them.
\section*{Problem 1}
Let $X_1,...,X_5$ be a random sample of size 5 from a normal population with mean $0 $and variance $1$ and let $\bar{X}=(1/5)\sum_{i=1}^{5}X_i^2$. Let $X_6$ be another independent observation from the same population. Find the distributions of
\begin{enumerate}
\item[A.] $W=\sum_{i=1}^5 X_i^2$
\item[B.] $U=\sum_{i=1}^5 (X_i-X_{\bar{X}})^2$
\item[C.] $T_1 = \sqrt{5} \frac{X_6}{\sqrt{W}}$
\item[D.] $T_2 = 2\frac{(5\bar{X}^2+(X_6)^2)}{U}$

\end{enumerate}
\subsection*{Part A}
$W$ simply represents the non averaged sum of the squares of the sample random variables.
\begin{align*}
W=\sum_{i=1}^5 X_i^2 = X_1^2+X_2^2+X_3^2+X_4^2+X_5^2
\end{align*}.
Here lets note that these are standard normal distributions. As such, the sum of their squares (i.e. $W$) will be a chi squared random variable with $5$ degrees of freedom.
\begin{align*}
W = \mathcal{X}_5^2
\end{align*}
\subsection*{Part B}
$U$ here is simply the sample Variance except missing the scaling $\frac{1}{n-1}$ factor.
\begin{align*}
S = \frac{1}{4} U && U = (4)S^2
\end{align*}
The sample variance for a sample of standard normal of sample size $5$ is:
\begin{align*}
S^2 \sim \frac{1}{4} \mathcal{X}_{4}^{2}
\end{align*}
As such, $U$ is then:
\begin{align*}
\boxed{ U = \mathcal{X}_{4}^{2} }
\end{align*}
\subsection*{Part C}
Finding $T_1$ should just be a case of simple substitution.
\begin{align*}
T_1 = \sqrt{5} \frac{X_6}{\sqrt{W}}
\end{align*}
Substituting in W from part A:
\begin{align*}
T_1 = \sqrt{5} \frac{X_6}{\sqrt{\mathcal{X}_5^2}}
\end{align*}
Note here that we can rewrite this fraction to move that factor of $\sqrt{5}$ from the numerator to the denominator.
\begin{align*}
T_1 = \frac{X_6}{\sqrt{\tfrac{\mathcal{X}_5^2}{5}}} 
\end{align*}
Here we should note that this is the exact form of Student's $T$ distribution. Thus $T_1$ is a T distribution with $5$ degrees of freedom.
\begin{align*}
\boxed{ T_1 \sim t_{5} }
\end{align*}
\subsection*{Part D}
Before proceeding, lets specify the distribution of $\bar{X}$.
\begin{align*}
\bar{X} \sim \mathcal{N}(0,\frac{1}{5})
\end{align*}
This result was dealt with extensively in part 1 of the last homework. With this in mind lets look at $T_2$
\begin{align*}
T_2 = 2\frac{(5\bar{X}^2+(X_6)^2)}{U}
\end{align*}
Here we have a sum of two squares of normal distributions. The former being nonstandard. But luckily that factor of $\sqrt{5}$ will help us out:
\begin{align*}
T_2 = 2\frac{(5\bar{X}^2+(X_6)^2)}{U} = 2\frac{((\sqrt{5}\bar{X})^2+X_6^2)}{U}
\end{align*}
The product of a scalar and a normal random variable is another gaussian. The mean scales by a factor of the scalar and the variance scales by the square of that scalar. As such $\sqrt{5}\bar{X} \sim \mathcal{N}(0,1)$. Therefore the square of $\sqrt{5}\bar{X}$ is a Chi-Squared random variable.
\begin{align*}
T_2 = 2\frac{(\mathcal{X}^2_1+(X_6)^2)}{U}
\end{align*}
We also know that $X_6$ is standard normal so it too is a Chi-Squared random variable but with 2 degrees of freedom.
\begin{align*}
T_2 = 2\frac{(\mathcal{X}^2_1+\mathcal{X}^2_1)}{U}
\end{align*}
These Chi-Squared can be combined:
\begin{align*}
T_2 = T_2 = \frac{2(\mathcal{X}^2_2)}{U}
\end{align*}
Now to substitute in $U$ from part C.
\begin{align*}
T_2 = \frac{2\mathcal{X}^2_5}{\mathcal{X}^2_4}
\end{align*}
Here we can rearrange the numerator like in part C to pull that factor of 4 into the denominator. However this time since we want $\tfrac{1}{4}$ in the denominator we must add a $\tfrac{1}{2}$ in the numerator to balance it out.
\begin{align*}
T_2 = \frac{\tfrac{\mathcal{X}^2_2}{2}}{\tfrac{\mathcal{X}^2_4}{4}}
\end{align*}
This here is an F distribution with $(2,4)$ degrees of freedom.
\begin{align*}
\boxed{T_2 \sim F(2,4)}
\end{align*}
\clearpage

\section*{Problem 2}
Show that the median of an F-distribution with $(\nu, \nu)$ degrees of freedom is 1. Also,
show that $Q_1Q_3 = 1$, where Q1 and Q3 are the first and third quartiles, respectively, of the $F(\nu,\nu)$
distribution.

Hint: You will have to use the fact that the reciprocal of an F random variable is also an F random
variable.

\subsection*{Part A}
First lets find the median of the F distribution with $(\nu, \nu)$ degrees of freedom. The median is defined as the value such that $\int_{-\infty}^{m} f_X(x) dx = \int_{m}^{\infty} f_X(x) dx = \frac{1}{2}$. Graphically it means that half the area under the pdf is to the left of the median, and half is to the right.

Before diving into computation, lets take a look at our distribution for a second, because we can exploit a property of it to make this easier. The F distribution is the fraction of two Chi squared random variables in this format:
\begin{align*}
F_{(n,m)} = \frac{\tfrac{\mathcal{X}_n^2}{n}}{\tfrac{\mathcal{X}_m^2}{m}}
\end{align*}
We are specifically dealing with the case where $m=n=\nu$. This is important because the following relation is then true:
\begin{align*}
F_{(\nu,\nu)} = \frac{\tfrac{\mathcal{X}_\nu^2}{\nu}}{\tfrac{\mathcal{X}^2_\nu}{\nu}} = F_{(\nu,\nu)}^{-1}
\end{align*}
If $F_{(\nu,\nu)}=F_{(\nu,\nu)}^{-1}$, we can establish this:
\begin{align*}
P(F(\nu,\nu)\geq m) = \frac{1}{2} && 1 - P(\frac{1}{F(\nu,\nu)} \geq \frac{1}{m}) = \frac{1}{2}
\end{align*}
From this we see thee relation $m=\tfrac{1}{m}$. Only one number satisfies this relation and that is $m=1$. Thus our median is:
\begin{align*}
\boxed{m=1}
\end{align*}
\subsection*{Part B}
Next is to find the first and third Quartiles. These are defined such that:
\begin{align*}
P(F(\nu,\nu) \leq Q_1) = 0.25 && P(F(\nu,\nu) \geq Q_3) = 0.25
\end{align*}
Since $\frac{1}{F(\nu,\nu)}=F(\nu,\nu)$, we can also say:
\begin{align*}
P(\frac{1}{F(\nu,\nu)} \leq Q_3) = 0.25
\end{align*}
Thus, with equality established between:
\begin{align*}
P(\frac{1}{F(\nu,\nu)} \leq Q_1) = P(\frac{1}{F(\nu,\nu)} \leq Q_1) = 0.25
\end{align*}
We know that 
\begin{align*}
\boxed{Q_1Q_3 = Q_1\frac{1}{Q_3} = 1}
\end{align*}

\clearpage
\section*{Problem 3}
Suppose $X_1,X_2,...,X_n$ be a random sample from an exponential distribution with mean $\theta$.
\begin{enumerate}
\item[A.] Derive the density for the smallest order statistic $X_{(1)}$. Identify the distribution.
\item[B.] Show that $X_{(1)}$ is independent of $X_{(n)}-X_{(1)}$.
\end{enumerate}
\subsection*{Part A}
Theorem 5.5.2 gives the pdf of the jth order statistic as:
\begin{align*}
f_{X_{(j)}}(x) = \frac{n!}{(j-1)!(n-j)!} f_X(x) [F_X(x)]^{j-1} [1-F_X(x)]^{n-j}
\end{align*}
In this case $j=1$, leaving us with:
\begin{align*}
f_{X_{(1)}}(x) = \frac{n!}{(n-1)!} f_X(x) [1-F_X(x)]^{n-1} = 1-[1-F(x)]^n
\end{align*}
Our pdf is the exponential distribution with mean $\theta$. As such we know the parameter $\lambda=\tfrac{1}{\theta}$. Our pdf is then given by:
\begin{align*}
f_X = \theta e^{-\tfrac{x}{\theta}} && 0 \leq x \leq \theta
\end{align*}
Our CDF is then:
\begin{align*}
F_X(x) = 1-e^{-\tfrac{x}{\theta}} && x \leq x \leq \theta
\end{align*}
Substituting this in:
\begin{align*}
f_{X_{(1)}}(x) = 1-[1-(1-e^{-\tfrac{x}{\theta}})]^n = 1-[e^{-\tfrac{x}{\theta}})]^n = 1-e^{-\tfrac{nx}{\theta}}
\end{align*}
Our CDF is then:
\begin{align*}
f_{X_{(1)}}(x) = 1-e^{-\tfrac{nx}{\theta}}
\end{align*}
This is actually the CDF of an exponential distribution with $\lambda = \tfrac{\theta}{n}$
\begin{align*}
\boxed{ X_{(1)} \sim \text{Exp}(\tfrac{\theta}{n}) }
\end{align*}
\subsection*{Part B}
Prior to attempting this problem, I was filled with dread imagining having to compute the transformation of $X_{(n)}-X_{(1)}$. Let $W=X_{(n)}-X_{(1)}$ Luckily we later went over this in class and it turns out this transformation turns out to be:
\begin{align*}
f_{W}(w) = n(n-1) \int_{-\infty}^{\infty} [F(w+t)-F(t)]^{n-2} f(w+t)f(t) dt
\end{align*}
Recall $F_X(x)=1-e^{-\frac{x}{\theta}}$ and $f_X(x)=\theta e^{-\frac{x}{\theta}}$. We can now substitute this in:
\begin{align*}
f_{W}(w) = n(n-1) \int_{-\infty}^{\infty} [1-e^{-\frac{w+t}{\theta}}-(1-e^{-\frac{t}{\theta}})]^{n-2} \theta^-1 e^{-\frac{w+t}{\theta}}\theta^{-1} e^{-\frac{t}{\theta}} dt
\end{align*}
Simplifying:
\begin{align*}
f_{W}(w) = \frac{n(n-1) e^{-\frac{w}{\theta}}}{\theta^2} \int_{0}^{\theta} [e^{-\tfrac{t}{\theta}}(1-e^{\tfrac{w}{\theta}})]^{n-2} e^{\tfrac{2t}{\theta}} dt = \frac{n(n-1) e^{-\frac{w}{\theta}}}{\theta^2} (1-e^{\tfrac{w}{\theta}}) \int_{0}^{\theta} e^{(\tfrac{4}{\theta}-\tfrac{n}{\theta})t} dt
\end{align*}
Distributing this:
\begin{align*}
f_{W}(w) = \frac{n(n-1) e^{-\frac{w}{\theta}}}{\theta^2} (1-e^{\tfrac{w}{\theta}}) \int_{0}^{\theta}e^{(\tfrac{4}{\theta}-\tfrac{n}{\theta})t} dt = \frac{n(n-1) e^{-\frac{w}{\theta}}}{\theta^2} (1-e^{\tfrac{w}{\theta}}) \frac{\theta - e^{4-n}\theta}{n-4}
\end{align*}

NOTE: I can't tell if I've made a mistake or if my setup is wrong. I am fairly certain the setup is correct but a mistake was made in integration. The way I would continue this problem depends on what form the distribution of the range $X_{(n)}-X_{(1)}$ ends up in. If it is in a recognizable form like a type of exponential, the sensible method would be to prove independence via the MGF method. Otherwise I would prove it via the distribution function method.

\clearpage

\section*{Problem 4}
 Let $X_1,...,X_n$ be a random sample of size $n$ from a population with pdf
 \begin{align*}
 f(x) = \begin{cases} 
      \frac{1}{\theta} & 0<x\leq \theta \\
       0 & \text{ otherwise } 
   \end{cases}
\end{align*}
Let $X_{(1)}<...< X_{(n)}$ be the order statistics. Show that $\frac{X_{(1)}}{X_{(n)}}$ and $X_{(n)}$ are independent
random variables.
\subsection*{Part A}
Its important to note that this is a uniform distribution with start point $0$ and endpoint $\theta$.
The pdf was given, and the cdf is known to be:
\begin{align*}
F_X = \frac{x}{\theta} && 0 \leq x \leq \theta
\end{align*}
The joint distribution between $X_1$ and $X_n$ is given by:
\begin{align*}
f_{X_{(1)},X_{(n)}} = \frac{n!}{(n-2)!} (\frac{1}{\theta^2}) [\frac{x_n}{\theta}-\frac{x_1}{\theta}]^{n-2} = n(n-1) \frac{(x_n-x_1)^{n-2}}{\theta^n}
\end{align*}
We can use a bivariate transformation to get the distribution of $frac{X_{(1)}}{X_{(n)}}$. Let:
\begin{align*}
U = X_{(n)} && V = \frac{X_{(1)}}{X_{(n)}} 
\end{align*}
From this:
\begin{align*}
X_{(1)} = UV && X_{(n)} = U 
\end{align*}
The transformed distribution is given by:
\begin{align*}
f_{U,V} = f_{X_{(1)},X_{(n)}}(h(u,v),g(u,v)) |J|
\end{align*}
$|J|$ in this case is given by
\begin{align*}
|J| = \begin{vmatrix}
v & u \\
1 & 0
\end{vmatrix} = U
\end{align*}
Substituting in what we know then:
\begin{align*}
f_{U,V}(u,v) = f_{X_{(1)},X_{(n)}}(h(u,w),g(u,w)) u = n(n-1)u \frac{(u-uv)^{n-2}}{\theta^n}
\end{align*}
If U and V are independant, then the joint distribution can be arranged such that $f_{U,V}(u,v)=f_u(u)f(v)$. As such, all we have to do now is factor this into two different functions.
\begin{align*}
f_{U}(u) = \frac{n(n-1)u^{n-1}}{\theta^n} \int_0^\theta (1-v)^{n-2} dv = \frac{n (n-1) (1-v)^{n-2}}{n}
\end{align*}
\begin{align*}
f_{V}(v) = \frac{n(n-1)(1-v)^{n-2}}{\theta^n} \int_0^\theta (u)^{n-1} du = n(1-v)^{n-2} \theta^{1-n}
\end{align*}
The product of these two distributions is:
\begin{align*}
f_{U}(u)f_{V}(v) = \frac{n u^{n-1}(n-1) (1-v)^{n-2} n(1-v)^{n-2} \theta}{n \theta^n} = \frac{u^{n-1}(n-1) (1-v)^{n-2} n(1-v)^{n-2}\theta}{\theta^n}
\end{align*}
Combining these disparate factors:
\begin{align*}
\boxed{ f_{U}(u)f_{V}(v) = n(n-1)u \frac{(u-uv)^{n-2}}{\theta^n} }
\end{align*}
Thus, we have proven the products of our margainal distributions equals our joint pdf which is the condition for independence.


\clearpage
\section*{Problem 5}
Suppose $X_1,...,X_n$ be a random sample from a $Beta(2,1)$ distribution.
\begin{enumerate}
\item[A.] Derive the density for the smallest order statistic $X_{(1)}$
\item[B.] Suppose $n = 3$. Compute the probability that $X_{(1)}$ exceeds the median of the distribution.
\item[C.] Again, let $n = 3$. What is the covariance between $X_{(2)}$ and $X_{(3)}$.

\end{enumerate}

\subsection*{Part A}
All $X_{(n)}$ samples are sampled from the $Beta(2,1)$ distribution. The pdf and cdf of this distribution is given by:
\begin{align*}
f_X(x) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)} = 2x & 0<x<1 && F_X(x) = B(2,1) = x^2
\end{align*}
Note here the CDF is the incomplete beta function. Theorem 5.5.2 gives the pdf of the jth order statistic as:
\begin{align*}
f_{X_{(j)}}(x) = \frac{n!}{(j-1)!(n-j)!} f_X(x) [F_X(x)]^{j-1} [1-F_X(x)]^{n-j}
\end{align*}
In this case $j=1$. Substituting in the CDF and PDF:
\begin{align*}
f_{X_{(j)}}(x) = \frac{n!}{(n-1)!} 2x [x^2]^{0} [1-x^2]^{n-1} = n 2x [1-x^2]^{n-1}
\end{align*}
Our density then is:
\begin{align*}
\boxed{ f_{X_{(j)}}(x) = n 2x (1-x^2)^{n-1} } && 0<x<1
\end{align*}
\subsection*{Part B}
With $n=3$, we have three Beta distributed random variables. I think its important to rephrase this question in easier to understand language. We have three random numbers with a beta distribution. What is the probability that the smallest of these numbers is greater than the median of the beta distribution? The Median of the beta distribution is $\tfrac{1}{\sqrt{2}}$.

First we can state our density with $n=3$
\begin{align*}
f_{X_{(j)}}(x) = (3) 2x (1-x^2)^{3-1} = 6x (1-x^2)^{2} && 0<x<1
\end{align*}
With this in mind, we need to solve for how much area under the curve from the median $\tfrac{1}{\sqrt{2}}$ to the end of the domain $1$. As such we get:
\begin{align*}
P(x_{(x_1)} > \tfrac{1}{\sqrt{2}}) = \int_{\tfrac{1}{\sqrt{2}}}^{1} 6x (1-x^2)^{2} dx
\end{align*}
This integral then evaluates to:
\begin{align*}
\boxed{ P(x_{(x_1)} > \tfrac{1}{\sqrt{2}}) = \frac{1}{8} }
\end{align*}
\subsection*{Part C}
First lets derive our distributions for the second and third order distributions. Since $n=3$ we can use the special case identity for its density:
\begin{align*}
f_{X_{(n)}}(x) = n[F(x)]^{n-1} f(x) = 3 [x^2]^2 2x = 6x^5 && 0 \leq x \leq 1 
\end{align*}
For the second distribution we must use the generalized formula and plug in $j=2$
\begin{align*}
f_{X_{(2)}}(x) = \frac{3!}{(2-1)!(3-2)!} f_X(x) [F_X(x)]^{2-1} [1-F_X(x)]^{3-2} = 6 2x [x^2] [1-x^2] = 12x^3[1-x^2]
\end{align*}
We also need the joint distribution of these two variables which is given by:
\begin{align*}
f_{X_{(n)},X_{(n)}}(y,z) = \frac{n!}{(j-1)!(k-j-1)!(n-k)!} [F(y)]^{j-1} [F(z)-F(y)]^{k-j-1} [1-F(z)]^{n-k} f(y) f(z)
\end{align*}
Plugging in what we know, where $j=2, k=n=3$:
\begin{align*}
f_{X_{(n)},X_{(n)}}(y,z) = \frac{3!}{(2-1)!(3-2-1)!(3-3)!} [F(y)]^{2-1} [F(z)-F(y)]^{3-2-1} [1-F(z)]^{3-3} f(y) f(z)
\end{align*}
\begin{align*}
f_{X_{(n)},X_{(n)}}(y,z) = 6 [y^2] 2y 2z = 24 y^3 z
\end{align*}
For ease of notation let $Y=X_{(2)}$ and $Z=X_{(n)}$. Covariance is given by:
\begin{align*}
Cov(Y,Z) = \mathbb{E}(YZ) - \mathbb{E}(Y)\mathbb{E}(Z)
\end{align*}
Our expectations are:
\begin{align*}
\mathbb{E}(YZ) =\frac{2}{3} && \mathbb{E}(Y) = \frac{24}{35} && \mathbb{E}(Z) = \frac{6}{7}
\end{align*}
Our covariance is then:
\begin{align*}
\boxed{ Cov(Y,Z) = \frac{2}{3} - \frac{24}{35}\frac{6}{7} = \frac{58}{735} = 0.0789 }
\end{align*}

\end{document}
